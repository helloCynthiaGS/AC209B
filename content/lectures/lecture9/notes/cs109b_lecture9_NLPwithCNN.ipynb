{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EMEXxtb2gqA",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Data Science 2: Advanced Topics in Data Science \n",
    "\n",
    "## Lecture 9: NLP example with CNN \n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2019**<br/>\n",
    "**Instructors:** Pavlos Protopapas and Mark Glickman<br/>\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JesjlGDmIMCo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this example, we will try to implement a CNN for text. \n",
    "\n",
    "We will use the task of IMDB movie review classification. Response variable is positive/negative review.\n",
    "\n",
    "A sentence can be thought of as a sequence of words which have semantic connections across time. \n",
    "\n",
    "By semantic connection, we mean that the words that occur earlier in the sentence influence the sentence's structure and meaning in the latter part of the sentence. \n",
    "\n",
    "Note: There are also semantic connections backwards in a sentence (we will revisit this idea when we do RNNs from both directions and combine their outputs which we will see in the next few lectures). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EWgRqV0lAgpc",
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, SimpleRNN\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.embeddings import Embedding\n",
    "import numpy as np\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GpiyvyQiAs5h",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**SEEDING - **\n",
    "Important thing to do in many machine learning tasks which involve stochastic sampling (where random numbers are generated for different samples) is to do seeding so that the results are fairly reproducible.\n",
    "\n",
    "**WHY SEEDING ?** Most random number generators in computers are pseudo-random number generators i.e. they generate random numbers starting from a seed, but internally have a deterministic formula to calculate the next random number it generates and thus, if you fix your seed, the set of random numbers produced are the same in every run.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b6oE6xofBX_n",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **STEP 1** : Load and visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5QpTzwZvBhhH",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We want to have a finite vocabulary (9,999 most frequent words, one for everything else)\n",
    "vocabulary_size = 10000\n",
    "\n",
    "#We also want to have a finite length of reviews and not have to process really long sentences.\n",
    "# Anything longer will be chopped! \n",
    "max_review_length = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uv_THMxbDSyu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For practical data science applications, we need to convert text into tokens since the machine understands only numbers and not really English words like humans can. As a simple example of tokenization, we can see a small example.\n",
    "\n",
    "Assume we have 5 sentences. This is how we tokenize them into numbers once we create a dictionary.\n",
    "\n",
    "1. I have books                       -                        [1, 4, 7]\n",
    "2. Interesting books are useful                     [10,2,9,8]\n",
    "3. I have computers                                        [1,4,6]\n",
    "4. Computers are interesting and useful     [6,9,11,10,8]\n",
    "5. Books and computers are both valuable. [7,10,2,9,13,12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uv_THMxbDSyu",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Create tokens for vocabulary based on frequency of occurrence.  Hence, we assign the following tokens\n",
    "\n",
    "I-1, books-2, computers-3, have-4, are-5, computers-6,books-7, useful-8, are-9, and-10,interesting-11, valuable-12, both-13\n",
    "\n",
    "\n",
    "Thankfully, in our dataset it is internally handled and each sentence is represented in such tokenized form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "icfiuXaoBdUD",
    "outputId": "5a6d4130-23c9-4d47-e595-b4ce7a1f731c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=vocabulary_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 110
    },
    "colab_type": "code",
    "id": "DfEBJe9vBVjO",
    "outputId": "57fc0997-a769-480f-da0f-95223a133f19",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews 25000\n",
      "Length of first and fifth review before padding 218 147\n",
      "First review [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "First label 1\n"
     ]
    }
   ],
   "source": [
    "print('Number of reviews', len(X_train))\n",
    "print('Length of first and fifth review before padding', len(X_train[0]) ,len(X_train[4]))\n",
    "print('First review', X_train[0])\n",
    "print('First label', y_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U1LWcZExHp3m",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pad sequences in order to ensure that all inputs have same sentence length and dimensions.\n",
    "\n",
    "<b> DISCUSSION </b> : Why are we padding here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "j1ibR_xWA5fK",
    "outputId": "624c4346-7552-4284-d903-6f21f7d68fef",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of first and fifth review after padding 500 500\n"
     ]
    }
   ],
   "source": [
    "X_train = sequence.pad_sequences(X_train, maxlen=max_review_length)\n",
    "X_test = sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
    "print('Length of first and fifth review after padding', len(X_train[0]) ,len(X_train[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SO00uQ7JvF6d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Is Accuracy the right metric to look at ? \n",
    "\n",
    "Discuss : In what cases is accuracy a good metric to measure classification models ? \n",
    "\n",
    "What other metrics are useful incase accuracy proves to be incompetent metric for our dataset ?  https://towardsdatascience.com/understanding-data-science-classification-metrics-in-scikit-learn-in-python-3bc336865019\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "mvzvoYnNvPm-",
    "outputId": "790dd79b-63b0-4759-816e-34d8bd74bb0a",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zeroes :  12500  and Number of ones :  12500\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "counts = dict(Counter(y_train))\n",
    "print('Number of zeroes : ', counts[0], ' and Number of ones : ', counts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_X0l7QDvlWMK",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MODEL 1(a) : FEEDFORWARD NETWORKS WITHOUT EMBEDDINGS \n",
    "\n",
    "Let us build a single layer feedforward net with 250 nodes.\n",
    "\n",
    "<b> GOAL </b> : Calculate the number of parameters involved in this network and implement a feedforward net to do classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "colab_type": "code",
    "id": "ZYaoE0fXkfcV",
    "outputId": "e4c4266f-38cf-418f-ccf8-22bd0579911f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 250)               125250    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 125,501\n",
      "Trainable params: 125,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/3\n",
      " - 1s - loss: 8.0534 - acc: 0.4990 - val_loss: 7.9612 - val_acc: 0.5053\n",
      "Epoch 2/3\n",
      " - 1s - loss: 8.0372 - acc: 0.5012 - val_loss: 8.0608 - val_acc: 0.4998\n",
      "Epoch 3/3\n",
      " - 1s - loss: 8.0574 - acc: 0.5001 - val_loss: 8.0603 - val_acc: 0.4999\n",
      "Accuracy: 49.99%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(250, activation='relu',input_dim=max_review_length))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UE1LtWH5lvWY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Any idea why the performance is terrible ?\n",
    "\n",
    "Hint : Tokenization. \n",
    "\n",
    "Obvious Workaround : One-Hot Encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "apqpdoncI0vl",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### EMBEDDINGS - Sparse to Dense Transformations\n",
    "\n",
    "We use embeddings to reduce dimensions of our data since the tokens we assign based on our word frequency are discrete and do not have a continuous structure.\n",
    "\n",
    "#### What are embeddings ?\n",
    "\n",
    "Embeddings are functional transformations from a sparse discrete vector representation of text (either as tokens or as one-hot encodings) into a dense vector representation of a fixed size(usually of much lower dimensions than the vocabulary length of the text). The dense representations allow the neural network to generalize better.\n",
    "\n",
    "Here we are training our own embedding while training our neural network. To transfer \"knowledge\" from other sources, in more complicated projects we can also use pre-trained embeddings such as word-2-vec, GloVE, Fastext etc.  https://nlpforhackers.io/word-embeddings/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hrFJQQP4gG-o",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example Embeddings Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qi8eHxEPfMJl",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us first understand how Keras embedding layer works through a dummy example to see how the dimensions are transformed. In this example, each input is mapped to a 64 dimensional vector (via the embedding layer).\n",
    "\n",
    "<b>EXERCISE</b> : Manually calculate the number of parameters needed in the embedding layer before executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217
    },
    "colab_type": "code",
    "id": "tmojGnFcfSfC",
    "outputId": "149f0cd8-49ba-4b76-f6bb-d1813bd63d7a",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 10, 64)            64000     \n",
      "=================================================================\n",
      "Total params: 64,000\n",
      "Trainable params: 64,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Shape of input :  (32, 10)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "#input - Number of categorical inputs, embedding dimension, input length.\n",
    "model.add(Embedding(1000, 64, input_length=10))\n",
    "print(model.summary())\n",
    "\n",
    "# the model will take as input an integer matrix of size (batch, input_length).\n",
    "# the largest integer (i.e. word index) in the input should be\n",
    "# no larger than 999 (vocabulary size).\n",
    "# now model.output_shape == (None, 10, 64), where None is the batch dimension.\n",
    "\n",
    "input_array = np.random.randint(1000, size=(32, 10))\n",
    "print(\"Shape of input : \", input_array.shape)\n",
    "model.compile('rmsprop', 'mse')\n",
    "output_array = model.predict(input_array)\n",
    "assert output_array.shape == (32, 10, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "E_hDBZ_rflhW",
    "outputId": "e756792a-f6a1-4835-f8c4-dc0e490c6f29",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[905 644  12 451 965 181  64 621 703 214]\n",
      "(10, 64)\n"
     ]
    }
   ],
   "source": [
    "print(input_array[0])\n",
    "print(output_array[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HXA43a_sgChz",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MODEL 1(b) : FEEDFORWARD NETWORKS WITH EMBEDDINGS \n",
    "\n",
    "<b>EXERCISE</b> : Implement the feedforward net combining the embedding layer and the feedforward layer(one layer, 250 nodes) without looking at cells below. Manually calculate the number of parameters needed in the feedforward network before executing the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZaOes1ab3Fs0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 308
    },
    "colab_type": "code",
    "id": "ODxwyYlYIyNb",
    "outputId": "44af240d-4855-4265-d404-2cb5d658c28c",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 500, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 50000)             0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 250)               12500250  \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 13,500,501\n",
      "Trainable params: 13,500,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "09CcheXuKfPn",
    "outputId": "909b1c33-9b47-410d-bb38-ccc5d33a134f",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 89s - loss: 0.5811 - acc: 0.6663 - val_loss: 0.3381 - val_acc: 0.8514\n",
      "Epoch 2/2\n",
      " - 84s - loss: 0.2002 - acc: 0.9222 - val_loss: 0.3017 - val_acc: 0.8726\n",
      "Accuracy: 87.26%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sEffI6gOK8cY",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### MODEL 2 : Convolutional Nets\n",
    "\n",
    "Text can be thought of as 1-dimensional sequence and we can apply 1-D Convolutions over a set of words. Let us walk through convolutions on text data with this blog.\n",
    "\n",
    "http://debajyotidatta.github.io/nlp/deep/learning/word-embeddings/2016/11/27/Understanding-Convolutions-In-Text/\n",
    "\n",
    "<b>EXERCISE</b> : Manually calculate the number of parameters needed in the feedforward network before executing the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "id": "ApWcBG8NK_2Q",
    "outputId": "637b1f63-6791-4f99-e738-6cefe99a493d",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 500, 100)          1000000   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 100)          30100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 250, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 25000)             0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 250)               6250250   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 251       \n",
      "=================================================================\n",
      "Total params: 7,280,601\n",
      "Trainable params: 7,280,601\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocabulary_size, embedding_dim, input_length=max_review_length))\n",
    "model.add(Conv1D(filters=embedding_dim, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(250, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "BWFN54kBLgpC",
    "outputId": "0cd9ee7f-36a3-41dd-ead2-5306dca4f939",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 25000 samples, validate on 25000 samples\n",
      "Epoch 1/2\n",
      " - 314s - loss: 0.4952 - acc: 0.7055 - val_loss: 0.2996 - val_acc: 0.8768\n",
      "Epoch 2/2\n",
      " - 147s - loss: 0.2043 - acc: 0.9216 - val_loss: 0.2849 - val_acc: 0.8819\n",
      "Accuracy: 88.19%\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)\n",
    "# Final evaluation of the model\n",
    "scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Accuracy: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U9EcCvXfjVxH",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### EXERCISE\n",
    "\n",
    "Try other CNNs with\n",
    "\n",
    "1. Different kernel sizes\n",
    "2. Different pooling operations(AveragePooling1D)\n",
    "\n",
    "<b>DISCUSSION</b> : What does max and average pooling mean in terms of processing text sequences ? "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "RNNs_sentiment_classification",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "livereveal": {
   "scroll": true
  },
  "rise": {
   "scroll": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
